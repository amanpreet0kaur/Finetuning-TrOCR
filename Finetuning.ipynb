{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12013843,"sourceType":"datasetVersion","datasetId":7558146},{"sourceId":12014728,"sourceType":"datasetVersion","datasetId":7558784},{"sourceId":12014828,"sourceType":"datasetVersion","datasetId":7558858},{"sourceId":12017429,"sourceType":"datasetVersion","datasetId":7560660}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers datasets\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T13:14:04.980532Z","iopub.execute_input":"2025-05-31T13:14:04.981244Z","iopub.status.idle":"2025-05-31T13:14:09.561059Z","shell.execute_reply.started":"2025-05-31T13:14:04.981217Z","shell.execute_reply":"2025-05-31T13:14:09.560154Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.9.0.13 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.4.0.6 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.10.19 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.4.40 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.9.5 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.9.41 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport pandas as pd\n\n# Parse labels.txt\nlabel_file = \"/kaggle/input/labeldata/label.txt\"\nimage_dir = \"/kaggle/input/studydata/input/dataset\"\n\ndata = []\nwith open(label_file, \"r\", encoding=\"utf-8\") as f:\n    for line in f:\n        fname, text = line.strip().split(\"\\t\")\n        img_path = os.path.join(image_dir, fname)\n        if os.path.exists(img_path):\n            data.append({\"image_path\": img_path, \"text\": text})\n\ndf = pd.DataFrame(data)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T13:14:09.562951Z","iopub.execute_input":"2025-05-31T13:14:09.563269Z","iopub.status.idle":"2025-05-31T13:14:10.112315Z","shell.execute_reply.started":"2025-05-31T13:14:09.563234Z","shell.execute_reply":"2025-05-31T13:14:10.111787Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"df.head(5)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T15:31:04.501319Z","iopub.execute_input":"2025-05-31T15:31:04.502047Z","iopub.status.idle":"2025-05-31T15:31:04.509624Z","shell.execute_reply.started":"2025-05-31T15:31:04.502023Z","shell.execute_reply":"2025-05-31T15:31:04.508893Z"}},"outputs":[{"execution_count":70,"output_type":"execute_result","data":{"text/plain":"                                          image_path                   text\n0  /kaggle/input/studydata/input/dataset/Picture1...             Why is it?\n1  /kaggle/input/studydata/input/dataset/Picture1...           How are you?\n2  /kaggle/input/studydata/input/dataset/Picture1...   Where are you going?\n3  /kaggle/input/studydata/input/dataset/Picture1...  This is questionable.\n4  /kaggle/input/studydata/input/dataset/Picture1...          Are you sure?","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_path</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/input/studydata/input/dataset/Picture1...</td>\n      <td>Why is it?</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/input/studydata/input/dataset/Picture1...</td>\n      <td>How are you?</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/input/studydata/input/dataset/Picture1...</td>\n      <td>Where are you going?</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/input/studydata/input/dataset/Picture1...</td>\n      <td>This is questionable.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/input/studydata/input/dataset/Picture1...</td>\n      <td>Are you sure?</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":70},{"cell_type":"code","source":"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n\nprocessor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\nmodel = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T13:14:10.138849Z","iopub.execute_input":"2025-05-31T13:14:10.139233Z","iopub.status.idle":"2025-05-31T13:14:47.239119Z","shell.execute_reply.started":"2025-05-31T13:14:10.139216Z","shell.execute_reply":"2025-05-31T13:14:47.238207Z"}},"outputs":[{"name":"stderr","text":"2025-05-31 13:14:21.287097: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748697261.504515      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748697261.575958      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/224 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c80b2c72b9e4b11ba867082743a2280"}},"metadata":{}},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.12k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcf93ebfe26e4dacb0596f3555f172dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"944aa5ade3eb47d19d7cb3f070a1d6f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdcd29057ded4527bc2b9e9e48135b78"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0af3444dbf3435d9a4c635b16599cc0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8dd92ccac0194badb9ad9f5b39860a40"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0e1f679e24b43d2a4e1830e394aa9dd"}},"metadata":{}},{"name":"stderr","text":"Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n  \"attention_probs_dropout_prob\": 0.0,\n  \"encoder_stride\": 16,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.0,\n  \"hidden_size\": 768,\n  \"image_size\": 384,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"model_type\": \"vit\",\n  \"num_attention_heads\": 12,\n  \"num_channels\": 3,\n  \"num_hidden_layers\": 12,\n  \"patch_size\": 16,\n  \"pooler_act\": \"tanh\",\n  \"pooler_output_size\": 768,\n  \"qkv_bias\": false,\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.51.3\"\n}\n\nConfig of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n  \"activation_dropout\": 0.0,\n  \"activation_function\": \"gelu\",\n  \"add_cross_attention\": true,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 0,\n  \"classifier_dropout\": 0.0,\n  \"cross_attention_hidden_size\": 768,\n  \"d_model\": 1024,\n  \"decoder_attention_heads\": 16,\n  \"decoder_ffn_dim\": 4096,\n  \"decoder_layerdrop\": 0.0,\n  \"decoder_layers\": 12,\n  \"decoder_start_token_id\": 2,\n  \"dropout\": 0.1,\n  \"eos_token_id\": 2,\n  \"init_std\": 0.02,\n  \"is_decoder\": true,\n  \"layernorm_embedding\": true,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"trocr\",\n  \"pad_token_id\": 1,\n  \"scale_embedding\": false,\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.51.3\",\n  \"use_cache\": false,\n  \"use_learned_position_embeddings\": true,\n  \"vocab_size\": 50265\n}\n\nSome weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69b76a390fb9428c996fdf091c22cd14"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"from datasets import Dataset\nfrom PIL import Image\nimport torch\n\n# Convert DataFrame to HuggingFace Dataset\ndataset = Dataset.from_pandas(df)\n\n# Preprocessing\ndef preprocess(example):\n    image = Image.open(example[\"image_path\"]).convert(\"RGB\")\n    pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values.squeeze(0)\n    \n    labels = processor.tokenizer(\n        example[\"text\"],\n        padding=\"max_length\",\n        truncation=True,\n        max_length=128,\n        return_tensors=\"pt\"\n    ).input_ids.squeeze(0)\n    labels[labels == processor.tokenizer.pad_token_id] = -100\n\n    return {\"pixel_values\": pixel_values, \"labels\": labels}\n\n# Map with proper formatting\ndataset = dataset.map(preprocess, remove_columns=dataset.column_names)\ndataset.set_format(type=\"torch\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T13:14:47.240386Z","iopub.execute_input":"2025-05-31T13:14:47.241095Z","iopub.status.idle":"2025-05-31T13:14:50.887448Z","shell.execute_reply.started":"2025-05-31T13:14:47.241065Z","shell.execute_reply":"2025-05-31T13:14:50.886604Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/86 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4cd71b66abad4be98d50603d1d20d507"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ndef collate_fn(batch):\n    pixel_values = torch.stack([x[\"pixel_values\"] for x in batch])\n    labels = torch.stack([x[\"labels\"] for x in batch])\n    return {\"pixel_values\": pixel_values, \"labels\": labels}\n\ntrain_dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T13:14:50.888544Z","iopub.execute_input":"2025-05-31T13:14:50.889339Z","iopub.status.idle":"2025-05-31T13:14:51.562239Z","shell.execute_reply.started":"2025-05-31T13:14:50.889317Z","shell.execute_reply":"2025-05-31T13:14:51.559800Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from datasets import Dataset\nfrom PIL import Image\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T13:14:51.563005Z","iopub.execute_input":"2025-05-31T13:14:51.563332Z","iopub.status.idle":"2025-05-31T13:14:52.439872Z","shell.execute_reply.started":"2025-05-31T13:14:51.563302Z","shell.execute_reply":"2025-05-31T13:14:52.439048Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\nmodel.config.pad_token_id = processor.tokenizer.pad_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T13:14:52.441327Z","iopub.execute_input":"2025-05-31T13:14:52.441673Z","iopub.status.idle":"2025-05-31T13:14:52.743356Z","shell.execute_reply.started":"2025-05-31T13:14:52.441642Z","shell.execute_reply":"2025-05-31T13:14:52.742558Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"from torch.optim import AdamW\nimport torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nmodel.train()\n\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\nbest_loss = float(\"inf\")  # Initialize with infinity\n\nfor epoch in range(100):  # Adjust as needed\n    total_loss = 0\n    model.train()\n    \n    for batch in train_dataloader:\n        pixel_values = batch[\"pixel_values\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        outputs = model(pixel_values=pixel_values, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n\n        optimizer.step()\n        optimizer.zero_grad()\n        total_loss += loss.item()\n\n    avg_loss = total_loss / len(train_dataloader)\n    print(f\"Epoch {epoch+1} Average Loss: {avg_loss:.4f}\")\n\n    # Save best model\n    if avg_loss < best_loss:\n        best_loss = avg_loss\n        print(f\"✅ New best model found at epoch {epoch+1} with loss {avg_loss:.4f}. Saving model...\")\n        model.save_pretrained(\"best-trocr-model\")\n        processor.save_pretrained(\"best-trocr-model\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T14:23:24.722812Z","iopub.execute_input":"2025-05-31T14:23:24.723140Z","iopub.status.idle":"2025-05-31T14:58:50.495872Z","shell.execute_reply.started":"2025-05-31T14:23:24.723119Z","shell.execute_reply":"2025-05-31T14:58:50.495157Z"}},"outputs":[{"name":"stdout","text":"Epoch 1 Average Loss: 0.3380\n✅ New best model found at epoch 1 with loss 0.3380. Saving model...\nEpoch 2 Average Loss: 0.3558\nEpoch 3 Average Loss: 0.2544\n✅ New best model found at epoch 3 with loss 0.2544. Saving model...\nEpoch 4 Average Loss: 0.1165\n✅ New best model found at epoch 4 with loss 0.1165. Saving model...\nEpoch 5 Average Loss: 0.1051\n✅ New best model found at epoch 5 with loss 0.1051. Saving model...\nEpoch 6 Average Loss: 0.1782\nEpoch 7 Average Loss: 0.1780\nEpoch 8 Average Loss: 0.4440\nEpoch 9 Average Loss: 0.2282\nEpoch 10 Average Loss: 0.1371\nEpoch 11 Average Loss: 0.2170\nEpoch 12 Average Loss: 0.1921\nEpoch 13 Average Loss: 0.1088\nEpoch 14 Average Loss: 0.0927\n✅ New best model found at epoch 14 with loss 0.0927. Saving model...\nEpoch 15 Average Loss: 0.1640\nEpoch 16 Average Loss: 0.2278\nEpoch 17 Average Loss: 0.1174\nEpoch 18 Average Loss: 0.1250\nEpoch 19 Average Loss: 0.0448\n✅ New best model found at epoch 19 with loss 0.0448. Saving model...\nEpoch 20 Average Loss: 0.0725\nEpoch 21 Average Loss: 0.0895\nEpoch 22 Average Loss: 0.1823\nEpoch 23 Average Loss: 0.0896\nEpoch 24 Average Loss: 0.0960\nEpoch 25 Average Loss: 0.1778\nEpoch 26 Average Loss: 0.1466\nEpoch 27 Average Loss: 0.0994\nEpoch 28 Average Loss: 0.0350\n✅ New best model found at epoch 28 with loss 0.0350. Saving model...\nEpoch 29 Average Loss: 0.0332\n✅ New best model found at epoch 29 with loss 0.0332. Saving model...\nEpoch 30 Average Loss: 0.0850\nEpoch 31 Average Loss: 0.0714\nEpoch 32 Average Loss: 0.0642\nEpoch 33 Average Loss: 0.1030\nEpoch 34 Average Loss: 0.3369\nEpoch 35 Average Loss: 0.3661\nEpoch 36 Average Loss: 0.4407\nEpoch 37 Average Loss: 0.3625\nEpoch 38 Average Loss: 0.3015\nEpoch 39 Average Loss: 0.1842\nEpoch 40 Average Loss: 0.1583\nEpoch 41 Average Loss: 0.1688\nEpoch 42 Average Loss: 0.0925\nEpoch 43 Average Loss: 0.1550\nEpoch 44 Average Loss: 0.1201\nEpoch 45 Average Loss: 0.0471\nEpoch 46 Average Loss: 0.0802\nEpoch 47 Average Loss: 0.0610\nEpoch 48 Average Loss: 0.0299\n✅ New best model found at epoch 48 with loss 0.0299. Saving model...\nEpoch 49 Average Loss: 0.0732\nEpoch 50 Average Loss: 0.3170\nEpoch 51 Average Loss: 0.3424\nEpoch 52 Average Loss: 0.6172\nEpoch 53 Average Loss: 0.3707\nEpoch 54 Average Loss: 0.3342\nEpoch 55 Average Loss: 0.3700\nEpoch 56 Average Loss: 0.1714\nEpoch 57 Average Loss: 0.0521\nEpoch 58 Average Loss: 0.0466\nEpoch 59 Average Loss: 0.0601\nEpoch 60 Average Loss: 0.0082\n✅ New best model found at epoch 60 with loss 0.0082. Saving model...\nEpoch 61 Average Loss: 0.0155\nEpoch 62 Average Loss: 0.0137\nEpoch 63 Average Loss: 0.0133\nEpoch 64 Average Loss: 0.0112\nEpoch 65 Average Loss: 0.0047\n✅ New best model found at epoch 65 with loss 0.0047. Saving model...\nEpoch 66 Average Loss: 0.0031\n✅ New best model found at epoch 66 with loss 0.0031. Saving model...\nEpoch 67 Average Loss: 0.0038\nEpoch 68 Average Loss: 0.0025\n✅ New best model found at epoch 68 with loss 0.0025. Saving model...\nEpoch 69 Average Loss: 0.0032\nEpoch 70 Average Loss: 0.0031\nEpoch 71 Average Loss: 0.0021\n✅ New best model found at epoch 71 with loss 0.0021. Saving model...\nEpoch 72 Average Loss: 0.0031\nEpoch 73 Average Loss: 0.0055\nEpoch 74 Average Loss: 0.0059\nEpoch 75 Average Loss: 0.0020\n✅ New best model found at epoch 75 with loss 0.0020. Saving model...\nEpoch 76 Average Loss: 0.0057\nEpoch 77 Average Loss: 0.0016\n✅ New best model found at epoch 77 with loss 0.0016. Saving model...\nEpoch 78 Average Loss: 0.0044\nEpoch 79 Average Loss: 0.0057\nEpoch 80 Average Loss: 0.0045\nEpoch 81 Average Loss: 0.0056\nEpoch 82 Average Loss: 0.0025\nEpoch 83 Average Loss: 0.0068\nEpoch 84 Average Loss: 0.0098\nEpoch 85 Average Loss: 0.0106\nEpoch 86 Average Loss: 0.0038\nEpoch 87 Average Loss: 0.0058\nEpoch 88 Average Loss: 0.0035\nEpoch 89 Average Loss: 0.0020\nEpoch 90 Average Loss: 0.0061\nEpoch 91 Average Loss: 0.0049\nEpoch 92 Average Loss: 0.0037\nEpoch 93 Average Loss: 0.0024\nEpoch 94 Average Loss: 0.0070\nEpoch 95 Average Loss: 0.0060\nEpoch 96 Average Loss: 0.0026\nEpoch 97 Average Loss: 0.0058\nEpoch 98 Average Loss: 0.0044\nEpoch 99 Average Loss: 0.0053\nEpoch 100 Average Loss: 0.0060\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"model.save_pretrained(\"finetuned-trocr-handwriting\")\nprocessor.save_pretrained(\"finetuned-trocr-handwriting\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T13:49:00.915322Z","iopub.execute_input":"2025-05-31T13:49:00.915541Z","iopub.status.idle":"2025-05-31T13:49:03.774912Z","shell.execute_reply.started":"2025-05-31T13:49:00.915525Z","shell.execute_reply":"2025-05-31T13:49:03.774294Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"[]"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"model.eval()\n\ntest_image = Image.open(\"/kaggle/input/studydata/input/dataset/Picture4.jpg\").convert(\"RGB\")\npixel_values = processor(images=test_image, return_tensors=\"pt\").pixel_values.to(device)\n\ngenerated_ids = model.generate(pixel_values)\npredicted_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\nprint(\"Predicted Text:\", predicted_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T15:30:05.315656Z","iopub.execute_input":"2025-05-31T15:30:05.315948Z","iopub.status.idle":"2025-05-31T15:30:05.780084Z","shell.execute_reply.started":"2025-05-31T15:30:05.315928Z","shell.execute_reply":"2025-05-31T15:30:05.779240Z"}},"outputs":[{"name":"stdout","text":"Predicted Text: This boring is.................\n","output_type":"stream"}],"execution_count":69},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_dir = \"/kaggle/working/finetuned-trocr-handwriting\"\nmodel.save_pretrained(model_dir)\nprocessor.save_pretrained(model_dir)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T14:01:22.241571Z","iopub.execute_input":"2025-05-31T14:01:22.241906Z","iopub.status.idle":"2025-05-31T14:01:26.264970Z","shell.execute_reply.started":"2025-05-31T14:01:22.241885Z","shell.execute_reply":"2025-05-31T14:01:26.264135Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"[]"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"from transformers import VisionEncoderDecoderModel, TrOCRProcessor\nfrom PIL import Image\nimport torch\n\n# Load your fine-tuned model and processor\nmodel = VisionEncoderDecoderModel.from_pretrained(\"finetuned-trocr-handwriting\")\nprocessor = TrOCRProcessor.from_pretrained(\"finetuned-trocr-handwriting\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T14:03:32.958771Z","iopub.execute_input":"2025-05-31T14:03:32.959316Z","iopub.status.idle":"2025-05-31T14:03:33.536145Z","shell.execute_reply.started":"2025-05-31T14:03:32.959295Z","shell.execute_reply":"2025-05-31T14:03:33.535360Z"}},"outputs":[{"name":"stderr","text":"Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n  \"attention_probs_dropout_prob\": 0.0,\n  \"encoder_stride\": 16,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.0,\n  \"hidden_size\": 768,\n  \"image_size\": 384,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"model_type\": \"vit\",\n  \"num_attention_heads\": 12,\n  \"num_channels\": 3,\n  \"num_hidden_layers\": 12,\n  \"patch_size\": 16,\n  \"pooler_act\": \"tanh\",\n  \"pooler_output_size\": 768,\n  \"qkv_bias\": false,\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.51.3\"\n}\n\nConfig of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n  \"activation_dropout\": 0.0,\n  \"activation_function\": \"gelu\",\n  \"add_cross_attention\": true,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 0,\n  \"classifier_dropout\": 0.0,\n  \"cross_attention_hidden_size\": 768,\n  \"d_model\": 1024,\n  \"decoder_attention_heads\": 16,\n  \"decoder_ffn_dim\": 4096,\n  \"decoder_layerdrop\": 0.0,\n  \"decoder_layers\": 12,\n  \"decoder_start_token_id\": 2,\n  \"dropout\": 0.1,\n  \"eos_token_id\": 2,\n  \"init_std\": 0.02,\n  \"is_decoder\": true,\n  \"layernorm_embedding\": true,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"trocr\",\n  \"pad_token_id\": 1,\n  \"scale_embedding\": false,\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.51.3\",\n  \"use_cache\": false,\n  \"use_learned_position_embeddings\": true,\n  \"vocab_size\": 50265\n}\n\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"import cv2\nimport numpy as np\nfrom PIL import Image\nimport os\n\ndef segment_lines(image_path):\n    # Read and convert image to grayscale\n    img = cv2.imread(image_path)\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n    # Threshold to binary\n    _, thresh = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY_INV)\n\n    # Use dilation to connect text lines\n    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, 10))\n    dilated = cv2.dilate(thresh, kernel, iterations=2)\n\n    # Find contours of lines\n    contours, _ = cv2.findContours(dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n    # Sort contours top to bottom\n    contours = sorted(contours, key=lambda c: cv2.boundingRect(c)[1])\n\n    # Crop line images\n    line_images = []\n    for c in contours:\n        x, y, w, h = cv2.boundingRect(c)\n        line_img = img[y:y+h, x:x+w]\n        line_images.append(Image.fromarray(line_img))\n\n    return line_images\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T14:18:55.748599Z","iopub.execute_input":"2025-05-31T14:18:55.748896Z","iopub.status.idle":"2025-05-31T14:18:55.755140Z","shell.execute_reply.started":"2025-05-31T14:18:55.748873Z","shell.execute_reply":"2025-05-31T14:18:55.754304Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"line_images = segment_lines(\"/kaggle/input/studydata/input/dataset/Picture14.jpg\")\n\nfull_text = \"\"\nfor line_img in line_images:\n    pixel_values = processor(images=line_img, return_tensors=\"pt\").pixel_values\n    generated_ids = model.generate(pixel_values)\n    text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n    full_text += text + \"\\n\"\n\nprint(\"Full Page Text:\\n\", full_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T15:27:56.333604Z","iopub.execute_input":"2025-05-31T15:27:56.334361Z","iopub.status.idle":"2025-05-31T15:27:56.492192Z","shell.execute_reply.started":"2025-05-31T15:27:56.334337Z","shell.execute_reply":"2025-05-31T15:27:56.491198Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/454980990.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline_img\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpixel_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mline_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mgenerated_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mfull_text\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2278\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_encoder_decoder\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"encoder_outputs\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2279\u001b[0m             \u001b[0;31m# if model is encoder decoder encoder_outputs are created and added to `model_kwargs`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2280\u001b[0;31m             model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(\n\u001b[0m\u001b[1;32m   2281\u001b[0m                 \u001b[0minputs_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_input_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneration_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2282\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_prepare_encoder_decoder_kwargs_for_generation\u001b[0;34m(self, inputs_tensor, model_kwargs, model_input_name, generation_config)\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0mencoder_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"return_dict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0mencoder_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_input_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m         \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoder_outputs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mModelOutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mencoder_kwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/vit/modeling_vit.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[1;32m    610\u001b[0m             \u001b[0mpixel_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpixel_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m         embedding_output = self.embeddings(\n\u001b[0m\u001b[1;32m    613\u001b[0m             \u001b[0mpixel_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool_masked_pos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool_masked_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolate_pos_encoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolate_pos_encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/vit/modeling_vit.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values, bool_masked_pos, interpolate_pos_encoding)\u001b[0m\n\u001b[1;32m    122\u001b[0m     ) -> torch.Tensor:\n\u001b[1;32m    123\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpixel_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolate_pos_encoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolate_pos_encoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbool_masked_pos\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/vit/modeling_vit.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values, interpolate_pos_encoding)\u001b[0m\n\u001b[1;32m    181\u001b[0m                     \u001b[0;34mf\" ({self.image_size[0]}*{self.image_size[1]}).\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                 )\n\u001b[0;32m--> 183\u001b[0;31m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[0;32m--> 549\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n","\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"],"ename":"RuntimeError","evalue":"Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor","output_type":"error"}],"execution_count":66}]}