{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12013843,"sourceType":"datasetVersion","datasetId":7558146},{"sourceId":12014728,"sourceType":"datasetVersion","datasetId":7558784},{"sourceId":12014828,"sourceType":"datasetVersion","datasetId":7558858},{"sourceId":12017429,"sourceType":"datasetVersion","datasetId":7560660}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers datasets\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T13:14:04.980532Z","iopub.execute_input":"2025-05-31T13:14:04.981244Z","iopub.status.idle":"2025-05-31T13:14:09.561059Z","shell.execute_reply.started":"2025-05-31T13:14:04.981217Z","shell.execute_reply":"2025-05-31T13:14:09.560154Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.9.0.13 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.4.0.6 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.10.19 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.4.40 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.9.5 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.9.41 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"**Creating a dataframe from the dataset**\nThis code reads label.txt, which contains tab-separated image filenames and their corresponding text labels. It builds full image paths, checks if they exist, and stores valid (image_path, text) pairs in a Pandas DataFrame.\n\nOutput: A DataFrame with columns:\n\nimage_path: Full path to each image\n\ntext: Corresponding label (transcription)\n","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\n\n# Parse labels.txt\nlabel_file = \"/kaggle/input/labeldata/label.txt\"\nimage_dir = \"/kaggle/input/studydata/input/dataset\"\n\ndata = []\nwith open(label_file, \"r\", encoding=\"utf-8\") as f:\n    for line in f:\n        fname, text = line.strip().split(\"\\t\")\n        img_path = os.path.join(image_dir, fname)\n        if os.path.exists(img_path):\n            data.append({\"image_path\": img_path, \"text\": text})\n\ndf = pd.DataFrame(data)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T13:14:09.562951Z","iopub.execute_input":"2025-05-31T13:14:09.563269Z","iopub.status.idle":"2025-05-31T13:14:10.112315Z","shell.execute_reply.started":"2025-05-31T13:14:09.563234Z","shell.execute_reply":"2025-05-31T13:14:10.111787Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"**Checking the dataframe**","metadata":{}},{"cell_type":"code","source":"df.head(50)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T16:08:50.814402Z","iopub.execute_input":"2025-05-31T16:08:50.814678Z","iopub.status.idle":"2025-05-31T16:08:50.823322Z","shell.execute_reply.started":"2025-05-31T16:08:50.814660Z","shell.execute_reply":"2025-05-31T16:08:50.822630Z"}},"outputs":[{"execution_count":79,"output_type":"execute_result","data":{"text/plain":"                                           image_path  \\\n0   /kaggle/input/studydata/input/dataset/Picture1...   \n1   /kaggle/input/studydata/input/dataset/Picture1...   \n2   /kaggle/input/studydata/input/dataset/Picture1...   \n3   /kaggle/input/studydata/input/dataset/Picture1...   \n4   /kaggle/input/studydata/input/dataset/Picture1...   \n5   /kaggle/input/studydata/input/dataset/Picture1...   \n6   /kaggle/input/studydata/input/dataset/Picture1...   \n7   /kaggle/input/studydata/input/dataset/Picture1...   \n8   /kaggle/input/studydata/input/dataset/Picture1...   \n9   /kaggle/input/studydata/input/dataset/Picture1...   \n10  /kaggle/input/studydata/input/dataset/Picture2...   \n11  /kaggle/input/studydata/input/dataset/Picture2...   \n12  /kaggle/input/studydata/input/dataset/Picture2...   \n13  /kaggle/input/studydata/input/dataset/Picture2...   \n14  /kaggle/input/studydata/input/dataset/Picture2...   \n15  /kaggle/input/studydata/input/dataset/Picture2...   \n16  /kaggle/input/studydata/input/dataset/Picture2...   \n17  /kaggle/input/studydata/input/dataset/Picture2...   \n18  /kaggle/input/studydata/input/dataset/Picture2...   \n19  /kaggle/input/studydata/input/dataset/Picture2...   \n20  /kaggle/input/studydata/input/dataset/Picture2...   \n21  /kaggle/input/studydata/input/dataset/Picture3...   \n22  /kaggle/input/studydata/input/dataset/Picture3...   \n23  /kaggle/input/studydata/input/dataset/Picture3...   \n24  /kaggle/input/studydata/input/dataset/Picture3...   \n25  /kaggle/input/studydata/input/dataset/Picture3...   \n26  /kaggle/input/studydata/input/dataset/Picture3...   \n27  /kaggle/input/studydata/input/dataset/Picture3...   \n28  /kaggle/input/studydata/input/dataset/Picture3...   \n29  /kaggle/input/studydata/input/dataset/Picture3...   \n30  /kaggle/input/studydata/input/dataset/Picture3...   \n31  /kaggle/input/studydata/input/dataset/Picture4...   \n32  /kaggle/input/studydata/input/dataset/Picture4...   \n33  /kaggle/input/studydata/input/dataset/Picture4...   \n34  /kaggle/input/studydata/input/dataset/Picture4...   \n35  /kaggle/input/studydata/input/dataset/Picture4...   \n36  /kaggle/input/studydata/input/dataset/Picture4...   \n37  /kaggle/input/studydata/input/dataset/Picture4...   \n38  /kaggle/input/studydata/input/dataset/Picture4...   \n39  /kaggle/input/studydata/input/dataset/Picture4...   \n40  /kaggle/input/studydata/input/dataset/Picture4...   \n41  /kaggle/input/studydata/input/dataset/Picture4...   \n42  /kaggle/input/studydata/input/dataset/Picture5...   \n43  /kaggle/input/studydata/input/dataset/Picture5...   \n44  /kaggle/input/studydata/input/dataset/Picture5...   \n45  /kaggle/input/studydata/input/dataset/Picture5...   \n46  /kaggle/input/studydata/input/dataset/Picture5...   \n47  /kaggle/input/studydata/input/dataset/Picture5...   \n48  /kaggle/input/studydata/input/dataset/Picture5...   \n49  /kaggle/input/studydata/input/dataset/Picture5...   \n\n                                            text  \n0                                     Why is it?  \n1                                   How are you?  \n2                           Where are you going?  \n3                          This is questionable.  \n4                                  Are you sure?  \n5                                      Cutie!!!!  \n6                                   How are you?  \n7                        It is gonna rain today.  \n8                      Are you going to library?  \n9        Automatically crops and resizes images.  \n10                                      No No No  \n11                    Saves labels in label.csv.  \n12      Your handwriting is neat and consistent.  \n13                          This is an icecream.  \n14                               Note that line.  \n15            Redundant instruction elimination.  \n16            Blocks GENERATE create transaction  \n17                        does not write program  \n18       This brown fox jumps iver the lazy dog.  \n19                            What is your name?  \n20                       This is my handwriting.  \n21                              We are new here.  \n22                  Name your file meaningfully.  \n23                 Resize each image to contain.  \n24                       Keep a labels.csv where  \n25             Transfer learning helps when data  \n26                      The number is 1234567890  \n27     A labeling script to match scanned images  \n28            Handwriting datasets are essential  \n29          Contains common punctuations/symbols  \n30                 The fish is found in the wild  \n31          The five boxing wizards jump quickly  \n32                               This is boring.  \n33       Artificial intelligence is transforming  \n34     These symbols :}.:&[)\"{+% are often used.  \n35                 use a flat ,well-lit surface.  \n36                Use a flat , well-lit surface.  \n37  The quick brown fox jumps over the lazy dog.  \n38                                  Pack my box.  \n39          She sells seashells by the seashore.  \n40         Bright vixens jumps; dozy fowl quack.  \n41         The five boxing wizards jump quickly.  \n42                     I don't wanna write more.  \n43                  Jackdows love my big sphinx.  \n44             We promptly judged antique ivory.  \n45        The job requires extra pluck and zeal.  \n46                     A mad boxer shot a quick.  \n47        How razorback-jumping frogs can level.  \n48          ML Models require clean labeled data  \n49                       Let's grab some coffee.  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_path</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/input/studydata/input/dataset/Picture1...</td>\n      <td>Why is it?</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/input/studydata/input/dataset/Picture1...</td>\n      <td>How are you?</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/input/studydata/input/dataset/Picture1...</td>\n      <td>Where are you going?</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/input/studydata/input/dataset/Picture1...</td>\n      <td>This is questionable.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/input/studydata/input/dataset/Picture1...</td>\n      <td>Are you sure?</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>/kaggle/input/studydata/input/dataset/Picture1...</td>\n      <td>Cutie!!!!</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>/kaggle/input/studydata/input/dataset/Picture1...</td>\n      <td>How are you?</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>/kaggle/input/studydata/input/dataset/Picture1...</td>\n      <td>It is gonna rain today.</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>/kaggle/input/studydata/input/dataset/Picture1...</td>\n      <td>Are you going to library?</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>/kaggle/input/studydata/input/dataset/Picture1...</td>\n      <td>Automatically crops and resizes images.</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>/kaggle/input/studydata/input/dataset/Picture2...</td>\n      <td>No No No</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>/kaggle/input/studydata/input/dataset/Picture2...</td>\n      <td>Saves labels in label.csv.</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>/kaggle/input/studydata/input/dataset/Picture2...</td>\n      <td>Your handwriting is neat and consistent.</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>/kaggle/input/studydata/input/dataset/Picture2...</td>\n      <td>This is an icecream.</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>/kaggle/input/studydata/input/dataset/Picture2...</td>\n      <td>Note that line.</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>/kaggle/input/studydata/input/dataset/Picture2...</td>\n      <td>Redundant instruction elimination.</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>/kaggle/input/studydata/input/dataset/Picture2...</td>\n      <td>Blocks GENERATE create transaction</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>/kaggle/input/studydata/input/dataset/Picture2...</td>\n      <td>does not write program</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>/kaggle/input/studydata/input/dataset/Picture2...</td>\n      <td>This brown fox jumps iver the lazy dog.</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>/kaggle/input/studydata/input/dataset/Picture2...</td>\n      <td>What is your name?</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>/kaggle/input/studydata/input/dataset/Picture2...</td>\n      <td>This is my handwriting.</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>/kaggle/input/studydata/input/dataset/Picture3...</td>\n      <td>We are new here.</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>/kaggle/input/studydata/input/dataset/Picture3...</td>\n      <td>Name your file meaningfully.</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>/kaggle/input/studydata/input/dataset/Picture3...</td>\n      <td>Resize each image to contain.</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>/kaggle/input/studydata/input/dataset/Picture3...</td>\n      <td>Keep a labels.csv where</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>/kaggle/input/studydata/input/dataset/Picture3...</td>\n      <td>Transfer learning helps when data</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>/kaggle/input/studydata/input/dataset/Picture3...</td>\n      <td>The number is 1234567890</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>/kaggle/input/studydata/input/dataset/Picture3...</td>\n      <td>A labeling script to match scanned images</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>/kaggle/input/studydata/input/dataset/Picture3...</td>\n      <td>Handwriting datasets are essential</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>/kaggle/input/studydata/input/dataset/Picture3...</td>\n      <td>Contains common punctuations/symbols</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>/kaggle/input/studydata/input/dataset/Picture3...</td>\n      <td>The fish is found in the wild</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>/kaggle/input/studydata/input/dataset/Picture4...</td>\n      <td>The five boxing wizards jump quickly</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>/kaggle/input/studydata/input/dataset/Picture4...</td>\n      <td>This is boring.</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>/kaggle/input/studydata/input/dataset/Picture4...</td>\n      <td>Artificial intelligence is transforming</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>/kaggle/input/studydata/input/dataset/Picture4...</td>\n      <td>These symbols :}.:&amp;[)\"{+% are often used.</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>/kaggle/input/studydata/input/dataset/Picture4...</td>\n      <td>use a flat ,well-lit surface.</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>/kaggle/input/studydata/input/dataset/Picture4...</td>\n      <td>Use a flat , well-lit surface.</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>/kaggle/input/studydata/input/dataset/Picture4...</td>\n      <td>The quick brown fox jumps over the lazy dog.</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>/kaggle/input/studydata/input/dataset/Picture4...</td>\n      <td>Pack my box.</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>/kaggle/input/studydata/input/dataset/Picture4...</td>\n      <td>She sells seashells by the seashore.</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>/kaggle/input/studydata/input/dataset/Picture4...</td>\n      <td>Bright vixens jumps; dozy fowl quack.</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>/kaggle/input/studydata/input/dataset/Picture4...</td>\n      <td>The five boxing wizards jump quickly.</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>/kaggle/input/studydata/input/dataset/Picture5...</td>\n      <td>I don't wanna write more.</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>/kaggle/input/studydata/input/dataset/Picture5...</td>\n      <td>Jackdows love my big sphinx.</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>/kaggle/input/studydata/input/dataset/Picture5...</td>\n      <td>We promptly judged antique ivory.</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>/kaggle/input/studydata/input/dataset/Picture5...</td>\n      <td>The job requires extra pluck and zeal.</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>/kaggle/input/studydata/input/dataset/Picture5...</td>\n      <td>A mad boxer shot a quick.</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>/kaggle/input/studydata/input/dataset/Picture5...</td>\n      <td>How razorback-jumping frogs can level.</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>/kaggle/input/studydata/input/dataset/Picture5...</td>\n      <td>ML Models require clean labeled data</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>/kaggle/input/studydata/input/dataset/Picture5...</td>\n      <td>Let's grab some coffee.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":79},{"cell_type":"markdown","source":"**This code loads the pretrained TrOCR model and its processor for handwritten text recognition.**\nThe TrOCRProcessor and VisionEncoderDecoderModel are key components used for handwriting recognition with the TrOCR model from Hugging Face. The TrOCRProcessor acts as a unified interface that combines a feature extractor and a tokenizer. It preprocesses input images by converting them into pixel values suitable for the vision encoder and also handles the tokenization of text for the decoder. The VisionEncoderDecoderModel is an end-to-end architecture that integrates a vision encoder (like ViT) and a text decoder (such as RoBERTa). In this setup, the encoder processes the input image to extract visual features, and the decoder generates the corresponding text output. Specifically, the \"microsoft/trocr-base-handwritten\" model is a pretrained version fine-tuned for recognizing handwritten text. Together, the processor and model enable seamless conversion of handwritten images into digital text through an image-to-sequence generation pipeline.","metadata":{}},{"cell_type":"code","source":"from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n\nprocessor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\nmodel = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T13:14:10.138849Z","iopub.execute_input":"2025-05-31T13:14:10.139233Z","iopub.status.idle":"2025-05-31T13:14:47.239119Z","shell.execute_reply.started":"2025-05-31T13:14:10.139216Z","shell.execute_reply":"2025-05-31T13:14:47.238207Z"}},"outputs":[{"name":"stderr","text":"2025-05-31 13:14:21.287097: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748697261.504515      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748697261.575958      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/224 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c80b2c72b9e4b11ba867082743a2280"}},"metadata":{}},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.12k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcf93ebfe26e4dacb0596f3555f172dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"944aa5ade3eb47d19d7cb3f070a1d6f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdcd29057ded4527bc2b9e9e48135b78"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0af3444dbf3435d9a4c635b16599cc0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8dd92ccac0194badb9ad9f5b39860a40"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0e1f679e24b43d2a4e1830e394aa9dd"}},"metadata":{}},{"name":"stderr","text":"Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n  \"attention_probs_dropout_prob\": 0.0,\n  \"encoder_stride\": 16,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.0,\n  \"hidden_size\": 768,\n  \"image_size\": 384,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"model_type\": \"vit\",\n  \"num_attention_heads\": 12,\n  \"num_channels\": 3,\n  \"num_hidden_layers\": 12,\n  \"patch_size\": 16,\n  \"pooler_act\": \"tanh\",\n  \"pooler_output_size\": 768,\n  \"qkv_bias\": false,\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.51.3\"\n}\n\nConfig of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n  \"activation_dropout\": 0.0,\n  \"activation_function\": \"gelu\",\n  \"add_cross_attention\": true,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 0,\n  \"classifier_dropout\": 0.0,\n  \"cross_attention_hidden_size\": 768,\n  \"d_model\": 1024,\n  \"decoder_attention_heads\": 16,\n  \"decoder_ffn_dim\": 4096,\n  \"decoder_layerdrop\": 0.0,\n  \"decoder_layers\": 12,\n  \"decoder_start_token_id\": 2,\n  \"dropout\": 0.1,\n  \"eos_token_id\": 2,\n  \"init_std\": 0.02,\n  \"is_decoder\": true,\n  \"layernorm_embedding\": true,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"trocr\",\n  \"pad_token_id\": 1,\n  \"scale_embedding\": false,\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.51.3\",\n  \"use_cache\": false,\n  \"use_learned_position_embeddings\": true,\n  \"vocab_size\": 50265\n}\n\nSome weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69b76a390fb9428c996fdf091c22cd14"}},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"This code converts a Pandas DataFrame containing image paths and corresponding text labels into a Hugging Face Dataset and applies preprocessing suitable for training a TrOCR model. Each image is loaded using PIL and processed into pixel_values using the TrOCRProcessor, which prepares the image input for the vision encoder. The associated text is tokenized into input_ids for the decoder, with padding token IDs replaced by -100 to ensure they are ignored during loss computation. The map() function applies this transformation to the entire dataset, and set_format(type=\"torch\") ensures that the output is returned as PyTorch tensors, ready for model training.","metadata":{}},{"cell_type":"code","source":"from datasets import Dataset\nfrom PIL import Image\nimport torch\n\n# Convert DataFrame to HuggingFace Dataset\ndataset = Dataset.from_pandas(df)\n\n# Preprocessing\ndef preprocess(example):\n    image = Image.open(example[\"image_path\"]).convert(\"RGB\")\n    pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values.squeeze(0)\n    \n    labels = processor.tokenizer(\n        example[\"text\"],\n        padding=\"max_length\",\n        truncation=True,\n        max_length=128,\n        return_tensors=\"pt\"\n    ).input_ids.squeeze(0)\n    labels[labels == processor.tokenizer.pad_token_id] = -100\n\n    return {\"pixel_values\": pixel_values, \"labels\": labels}\n\n# Map with proper formatting\ndataset = dataset.map(preprocess, remove_columns=dataset.column_names)\ndataset.set_format(type=\"torch\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T13:14:47.240386Z","iopub.execute_input":"2025-05-31T13:14:47.241095Z","iopub.status.idle":"2025-05-31T13:14:50.887448Z","shell.execute_reply.started":"2025-05-31T13:14:47.241065Z","shell.execute_reply":"2025-05-31T13:14:50.886604Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/86 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4cd71b66abad4be98d50603d1d20d507"}},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"This code sets up a PyTorch DataLoader to feed preprocessed image-text data into the TrOCR model during training. A custom collate_fn is defined to correctly batch the pixel_values and labels from individual examples by stacking them into tensors. The DataLoader is then created using the processed Hugging Face dataset, with a batch size of 4, shuffling enabled for training randomness, and the custom collate function to handle batching. This setup ensures efficient and correctly formatted input to the model.","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ndef collate_fn(batch):\n    pixel_values = torch.stack([x[\"pixel_values\"] for x in batch])\n    labels = torch.stack([x[\"labels\"] for x in batch])\n    return {\"pixel_values\": pixel_values, \"labels\": labels}\n\ntrain_dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T13:14:50.888544Z","iopub.execute_input":"2025-05-31T13:14:50.889339Z","iopub.status.idle":"2025-05-31T13:14:51.562239Z","shell.execute_reply.started":"2025-05-31T13:14:50.889317Z","shell.execute_reply":"2025-05-31T13:14:51.559800Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from datasets import Dataset\nfrom PIL import Image\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T13:14:51.563005Z","iopub.execute_input":"2025-05-31T13:14:51.563332Z","iopub.status.idle":"2025-05-31T13:14:52.439872Z","shell.execute_reply.started":"2025-05-31T13:14:51.563302Z","shell.execute_reply":"2025-05-31T13:14:52.439048Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"**Initializing decoder**","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\nmodel.config.pad_token_id = processor.tokenizer.pad_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T13:14:52.441327Z","iopub.execute_input":"2025-05-31T13:14:52.441673Z","iopub.status.idle":"2025-05-31T13:14:52.743356Z","shell.execute_reply.started":"2025-05-31T13:14:52.441642Z","shell.execute_reply":"2025-05-31T13:14:52.742558Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"**Training the model on our dataset**","metadata":{}},{"cell_type":"code","source":"from torch.optim import AdamW\nimport torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nmodel.train()\n\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\nbest_loss = float(\"inf\")  # Initialize with infinity\n\nfor epoch in range(100):  # Adjust as needed\n    total_loss = 0\n    model.train()\n    \n    for batch in train_dataloader:\n        pixel_values = batch[\"pixel_values\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        outputs = model(pixel_values=pixel_values, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n\n        optimizer.step()\n        optimizer.zero_grad()\n        total_loss += loss.item()\n\n    avg_loss = total_loss / len(train_dataloader)\n    print(f\"Epoch {epoch+1} Average Loss: {avg_loss:.4f}\")\n\n    # Save best model\n    if avg_loss < best_loss:\n        best_loss = avg_loss\n        print(f\"✅ New best model found at epoch {epoch+1} with loss {avg_loss:.4f}. Saving model...\")\n        model.save_pretrained(\"best-trocr-model\")\n        processor.save_pretrained(\"best-trocr-model\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T14:23:24.722812Z","iopub.execute_input":"2025-05-31T14:23:24.723140Z","iopub.status.idle":"2025-05-31T14:58:50.495872Z","shell.execute_reply.started":"2025-05-31T14:23:24.723119Z","shell.execute_reply":"2025-05-31T14:58:50.495157Z"}},"outputs":[{"name":"stdout","text":"Epoch 1 Average Loss: 0.3380\n✅ New best model found at epoch 1 with loss 0.3380. Saving model...\nEpoch 2 Average Loss: 0.3558\nEpoch 3 Average Loss: 0.2544\n✅ New best model found at epoch 3 with loss 0.2544. Saving model...\nEpoch 4 Average Loss: 0.1165\n✅ New best model found at epoch 4 with loss 0.1165. Saving model...\nEpoch 5 Average Loss: 0.1051\n✅ New best model found at epoch 5 with loss 0.1051. Saving model...\nEpoch 6 Average Loss: 0.1782\nEpoch 7 Average Loss: 0.1780\nEpoch 8 Average Loss: 0.4440\nEpoch 9 Average Loss: 0.2282\nEpoch 10 Average Loss: 0.1371\nEpoch 11 Average Loss: 0.2170\nEpoch 12 Average Loss: 0.1921\nEpoch 13 Average Loss: 0.1088\nEpoch 14 Average Loss: 0.0927\n✅ New best model found at epoch 14 with loss 0.0927. Saving model...\nEpoch 15 Average Loss: 0.1640\nEpoch 16 Average Loss: 0.2278\nEpoch 17 Average Loss: 0.1174\nEpoch 18 Average Loss: 0.1250\nEpoch 19 Average Loss: 0.0448\n✅ New best model found at epoch 19 with loss 0.0448. Saving model...\nEpoch 20 Average Loss: 0.0725\nEpoch 21 Average Loss: 0.0895\nEpoch 22 Average Loss: 0.1823\nEpoch 23 Average Loss: 0.0896\nEpoch 24 Average Loss: 0.0960\nEpoch 25 Average Loss: 0.1778\nEpoch 26 Average Loss: 0.1466\nEpoch 27 Average Loss: 0.0994\nEpoch 28 Average Loss: 0.0350\n✅ New best model found at epoch 28 with loss 0.0350. Saving model...\nEpoch 29 Average Loss: 0.0332\n✅ New best model found at epoch 29 with loss 0.0332. Saving model...\nEpoch 30 Average Loss: 0.0850\nEpoch 31 Average Loss: 0.0714\nEpoch 32 Average Loss: 0.0642\nEpoch 33 Average Loss: 0.1030\nEpoch 34 Average Loss: 0.3369\nEpoch 35 Average Loss: 0.3661\nEpoch 36 Average Loss: 0.4407\nEpoch 37 Average Loss: 0.3625\nEpoch 38 Average Loss: 0.3015\nEpoch 39 Average Loss: 0.1842\nEpoch 40 Average Loss: 0.1583\nEpoch 41 Average Loss: 0.1688\nEpoch 42 Average Loss: 0.0925\nEpoch 43 Average Loss: 0.1550\nEpoch 44 Average Loss: 0.1201\nEpoch 45 Average Loss: 0.0471\nEpoch 46 Average Loss: 0.0802\nEpoch 47 Average Loss: 0.0610\nEpoch 48 Average Loss: 0.0299\n✅ New best model found at epoch 48 with loss 0.0299. Saving model...\nEpoch 49 Average Loss: 0.0732\nEpoch 50 Average Loss: 0.3170\nEpoch 51 Average Loss: 0.3424\nEpoch 52 Average Loss: 0.6172\nEpoch 53 Average Loss: 0.3707\nEpoch 54 Average Loss: 0.3342\nEpoch 55 Average Loss: 0.3700\nEpoch 56 Average Loss: 0.1714\nEpoch 57 Average Loss: 0.0521\nEpoch 58 Average Loss: 0.0466\nEpoch 59 Average Loss: 0.0601\nEpoch 60 Average Loss: 0.0082\n✅ New best model found at epoch 60 with loss 0.0082. Saving model...\nEpoch 61 Average Loss: 0.0155\nEpoch 62 Average Loss: 0.0137\nEpoch 63 Average Loss: 0.0133\nEpoch 64 Average Loss: 0.0112\nEpoch 65 Average Loss: 0.0047\n✅ New best model found at epoch 65 with loss 0.0047. Saving model...\nEpoch 66 Average Loss: 0.0031\n✅ New best model found at epoch 66 with loss 0.0031. Saving model...\nEpoch 67 Average Loss: 0.0038\nEpoch 68 Average Loss: 0.0025\n✅ New best model found at epoch 68 with loss 0.0025. Saving model...\nEpoch 69 Average Loss: 0.0032\nEpoch 70 Average Loss: 0.0031\nEpoch 71 Average Loss: 0.0021\n✅ New best model found at epoch 71 with loss 0.0021. Saving model...\nEpoch 72 Average Loss: 0.0031\nEpoch 73 Average Loss: 0.0055\nEpoch 74 Average Loss: 0.0059\nEpoch 75 Average Loss: 0.0020\n✅ New best model found at epoch 75 with loss 0.0020. Saving model...\nEpoch 76 Average Loss: 0.0057\nEpoch 77 Average Loss: 0.0016\n✅ New best model found at epoch 77 with loss 0.0016. Saving model...\nEpoch 78 Average Loss: 0.0044\nEpoch 79 Average Loss: 0.0057\nEpoch 80 Average Loss: 0.0045\nEpoch 81 Average Loss: 0.0056\nEpoch 82 Average Loss: 0.0025\nEpoch 83 Average Loss: 0.0068\nEpoch 84 Average Loss: 0.0098\nEpoch 85 Average Loss: 0.0106\nEpoch 86 Average Loss: 0.0038\nEpoch 87 Average Loss: 0.0058\nEpoch 88 Average Loss: 0.0035\nEpoch 89 Average Loss: 0.0020\nEpoch 90 Average Loss: 0.0061\nEpoch 91 Average Loss: 0.0049\nEpoch 92 Average Loss: 0.0037\nEpoch 93 Average Loss: 0.0024\nEpoch 94 Average Loss: 0.0070\nEpoch 95 Average Loss: 0.0060\nEpoch 96 Average Loss: 0.0026\nEpoch 97 Average Loss: 0.0058\nEpoch 98 Average Loss: 0.0044\nEpoch 99 Average Loss: 0.0053\nEpoch 100 Average Loss: 0.0060\n","output_type":"stream"}],"execution_count":55},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"**Saving the model**","metadata":{}},{"cell_type":"code","source":"model.save_pretrained(\"finetuned-trocr-handwriting\")\nprocessor.save_pretrained(\"finetuned-trocr-handwriting\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T13:49:00.915322Z","iopub.execute_input":"2025-05-31T13:49:00.915541Z","iopub.status.idle":"2025-05-31T13:49:03.774912Z","shell.execute_reply.started":"2025-05-31T13:49:00.915525Z","shell.execute_reply":"2025-05-31T13:49:03.774294Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"[]"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"**Prediction**","metadata":{}},{"cell_type":"code","source":"model.eval()\n\ntest_image = Image.open(\"/kaggle/input/studydata/input/dataset/Picture23.jpg\").convert(\"RGB\")\npixel_values = processor(images=test_image, return_tensors=\"pt\").pixel_values.to(device)\n\ngenerated_ids = model.generate(pixel_values)\npredicted_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\nprint(\"Predicted Text:\", predicted_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T16:09:21.875920Z","iopub.execute_input":"2025-05-31T16:09:21.876186Z","iopub.status.idle":"2025-05-31T16:09:22.025123Z","shell.execute_reply.started":"2025-05-31T16:09:21.876167Z","shell.execute_reply":"2025-05-31T16:09:22.024449Z"}},"outputs":[{"name":"stdout","text":"Predicted Text: Note that line\n","output_type":"stream"}],"execution_count":82},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"**Save model in kaggle**","metadata":{}},{"cell_type":"code","source":"model_dir = \"/kaggle/working/finetuned-trocr-handwriting\"\nmodel.save_pretrained(model_dir)\nprocessor.save_pretrained(model_dir)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T14:01:22.241571Z","iopub.execute_input":"2025-05-31T14:01:22.241906Z","iopub.status.idle":"2025-05-31T14:01:26.264970Z","shell.execute_reply.started":"2025-05-31T14:01:22.241885Z","shell.execute_reply":"2025-05-31T14:01:26.264135Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"[]"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"from transformers import VisionEncoderDecoderModel, TrOCRProcessor\nfrom PIL import Image\nimport torch\n\n# Load your fine-tuned model and processor\nmodel = VisionEncoderDecoderModel.from_pretrained(\"finetuned-trocr-handwriting\")\nprocessor = TrOCRProcessor.from_pretrained(\"finetuned-trocr-handwriting\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T14:03:32.958771Z","iopub.execute_input":"2025-05-31T14:03:32.959316Z","iopub.status.idle":"2025-05-31T14:03:33.536145Z","shell.execute_reply.started":"2025-05-31T14:03:32.959295Z","shell.execute_reply":"2025-05-31T14:03:33.535360Z"}},"outputs":[{"name":"stderr","text":"Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n  \"attention_probs_dropout_prob\": 0.0,\n  \"encoder_stride\": 16,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.0,\n  \"hidden_size\": 768,\n  \"image_size\": 384,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"model_type\": \"vit\",\n  \"num_attention_heads\": 12,\n  \"num_channels\": 3,\n  \"num_hidden_layers\": 12,\n  \"patch_size\": 16,\n  \"pooler_act\": \"tanh\",\n  \"pooler_output_size\": 768,\n  \"qkv_bias\": false,\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.51.3\"\n}\n\nConfig of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n  \"activation_dropout\": 0.0,\n  \"activation_function\": \"gelu\",\n  \"add_cross_attention\": true,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 0,\n  \"classifier_dropout\": 0.0,\n  \"cross_attention_hidden_size\": 768,\n  \"d_model\": 1024,\n  \"decoder_attention_heads\": 16,\n  \"decoder_ffn_dim\": 4096,\n  \"decoder_layerdrop\": 0.0,\n  \"decoder_layers\": 12,\n  \"decoder_start_token_id\": 2,\n  \"dropout\": 0.1,\n  \"eos_token_id\": 2,\n  \"init_std\": 0.02,\n  \"is_decoder\": true,\n  \"layernorm_embedding\": true,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"trocr\",\n  \"pad_token_id\": 1,\n  \"scale_embedding\": false,\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.51.3\",\n  \"use_cache\": false,\n  \"use_learned_position_embeddings\": true,\n  \"vocab_size\": 50265\n}\n\n","output_type":"stream"}],"execution_count":31}]}